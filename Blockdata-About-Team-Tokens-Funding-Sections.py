# -*- coding: utf-8 -*-
"""Blockdata_webscraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LbrVW-E0Rf2lYZlXM83YEeC6xVfzkjDZ
"""
# This code uses beautifulsoup and playwright for browser automation
import requests
from bs4 import BeautifulSoup
import csv
import lxml
import time
from playwright.async_api import async_playwright
import asyncio

# Make an HTTP GET request to the main page URL
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.1234.56 Safari/537.36",
    "Referer": "https://www.example.com",
    "Accept-Language": "en-US,en;q=0.9"
}
url = "https://www.blockdata.tech/markets/use-cases"

response = requests.get(url, headers=headers)

# Parse the HTML content using Beautiful Soup
soup = BeautifulSoup(response.content, "lxml")

# Find the containers that hold the provider information
a_tags = soup.find_all('a')


links = [a["href"] for a in a_tags]

filtered_links = [link for link in links if link.startswith("use-cases")]

base_url_for_use_cases = 'https://www.blockdata.tech/markets/'
filename = 'all_use_cases_sheet.csv'
with open(filename, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Base Url','Use Case'])

    for use_case in filtered_links:
        writer.writerow([base_url_for_use_cases, use_case])

# So now - beginning of the step where i want to append the use-cases to the base url and iterate over the pages to obtain the links to the profiles.
base_url = 'https://www.blockdata.tech/markets/'
use_cases_url_list = []

# Original code
for use_case in filtered_links:
  url = base_url + use_case
  use_cases_url_list.append(url)

# # So at this point now want to make requests to the use-cases page to scrap the profile links
profiles_list = []
for use_case_url in use_cases_url_list:
  response = requests.get(use_case_url, headers=headers)
  soup = BeautifulSoup(response.content, "lxml")
  prettified_html = soup.prettify()
  link_tags = soup.find_all("a", class_="MuiButton-root MuiButton-outlined MuiButton-outlinedPrimary MuiButton-sizeMedium MuiButton-outlinedSizeMedium MuiButtonBase-root index-button css-sgl86c")
  links = [a["href"] for a in link_tags]
  for link in links:
    if link not in profiles_list:
      profiles_list.append(link)
print(profiles_list)


# # # Writing data to the csv file
base_url_for_profiles = 'https://www.blockdata.tech'
with open("all_profiles_data.csv", "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["Base URL", "Link"])  # Write the header row
    for link in profiles_list:
        writer.writerow([base_url_for_profiles, link])

import time
test_profiles_list_urls = ['/profiles/hex-trust']

# So i need to write data to several csv files
# 1 - Blockdata-about-section (Already have this one)
# 2 - Team section (Just gonna save the profile link and Name, and rest of team section)
# 3 - Customers / Partners section
# 4 - Products section csv
# 5 - Funding section
# 6 - News section

about_section_sheet = 'Blockdata-about-section.csv'
team_section_sheet = 'Blockdata-team-section.csv'
funding_section_sheet = 'Blockdata-funding-section.csv'
tokens_sheet = 'Blockdata-tokens-section.csv'

# Initialising a csv writer to write to the following files
with open(about_section_sheet, "w", encoding='utf-8', newline="") as about_section_csv, \
     open(team_section_sheet, "w", encoding='utf-8', newline="") as team_section_csv, \
     open(funding_section_sheet, "w",encoding='utf-8', newline="") as funding_section_csv, \
     open(tokens_sheet, "w",encoding='utf-8', newline="") as tokens_sheet_csv:

    about_section_writer = csv.writer(about_section_csv, escapechar='\\')
    about_section_writer.writerow(["Profile Link","Name","Profile Bio", "Logo Url", "Caption", "Description", "Based In", "Website Url", "LinkedIn", "Twitter", "Facebook", "Telegram", "Github", "Reddit",
                     "Medium", "Total Amount Raised", "Team Size", "Number Of Clients", "Year of establishment", "Profile Type", "Status" , "Key Objective",
                     "Slogan / Motto", "Use cases", "Parent Organizations","Sub Organization", "Registered Entity Name"]) #Writing the header row

    team_section_writer = csv.writer(team_section_csv, escapechar='\\')
    team_section_writer.writerow(["Profile Link", "Name", "Team member, Responsibility and LinkedIn Profile"])  #Writing the header row

    funding_section_writer = csv.writer(funding_section_csv,escapechar='\\')
    funding_section_writer.writerow(["Profile Link", "Name", "Total Amount Raised", "Received Funding Rounds", "Total Amount Of Investors", "List Of Received Funding",
       "Investment Rounds", "Total Amount Of Profiles", "List Of Investments"])

    tokens_section_writer = csv.writer(tokens_sheet_csv,escapechar='\\')
    tokens_section_writer.writerow(["Profile Link", "Name", "Token Name","Token Symbol", "Type", "Runs on", "Token Standards", "Price", "Market Cap", "Link To Token" ])

    # The for loop will send request to each and every profile which is in the 'profiles_list' list
    # Will also print to the console index and url of the current index its at so that developer can see progress of the loop
    for index, url in enumerate(profiles_list):
      print(f"{index} - {url}")
      full_endpoint = base_url_for_profiles + url
      response = requests.get(full_endpoint, headers=headers)
      soup = BeautifulSoup(response.content, "lxml")
      prettified_html = soup.prettify()

      # Extract the string company name for example "sweatcoin || coinbase" from the test_url
      string_to_extract = url.split('/')[-1]
      extracted_string = string_to_extract.capitalize()

      #0 - The Profile Link
      profile_link = base_url_for_profiles + url  #here as the url are stored just as the last part. They are appended to the base url to make a full one

      # Now writing the code to extract different data points in the response html
      # 1 - The image tag
      # Extracting the src url in the img tag
      image_tag = soup.find('img', alt=extracted_string)  

      if image_tag:
        # Extract the image source attribute
        image_url = image_tag['src']
      else:
        image_url = 'N/A'

      # 2 - Company name
      header_tag = soup.find('h1', class_="MuiTypography-root MuiTypography-h3 css-1vj7cda")
      if header_tag:
        company_name = header_tag.get_text()
      else:
        company_name = 'N/A'

      # 3 - Caption
      span_tag = soup.find('span', class_="MuiTypography-root MuiTypography-caption css-2wkurb")
      if span_tag:
        company_caption = span_tag.get_text()
      else:
        company_caption = 'N/A'

      # 4 - Company description
      description_p_tag = soup.find('p', class_="MuiTypography-root MuiTypography-body1 css-yt6mq")
      if description_p_tag:
        company_description = description_p_tag.get_text()
      else:
        company_description = 'N/A'

      # 5 - Based in
      p_tag = soup.find('p', class_="MuiTypography-root MuiTypography-body1 css-1cghcd7")
      if p_tag:
        based_in = p_tag.get_text()
      else:
        based_in = 'N/A'

      # 6 - Website url
      a_tag = soup.find('a', class_="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorPrimary MuiIconButton-sizeMedium css-6n14gx")
      if a_tag:
        website_url = a_tag["href"]
      else:
        website_url = 'N/A'


      #7 - Linked in profile
      a_tag = soup.find('a', class_="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorPrimary MuiIconButton-sizeLarge e8femqp0 css-racc64")
      if a_tag:
        linked_in_profile = a_tag["href"]
      else:
        linked_in_profile = 'N/A'

      # 8 - Twitter link
      a_tag = soup.find('a', class_="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorPrimary MuiIconButton-sizeSmall e8femqp0 css-103bd9p")
      if a_tag:
        twitter_handle = a_tag["href"]
      else:
        twitter_handle = 'N/A'

      # 9 - Facebook link
      a_tag = soup.find('a', attrs={'aria-label': 'Facebook'})

      if a_tag:
        facebook_page = a_tag.get('href')
      else:
        facebook_page = 'N/A'

      # 10 - Telegram
      a_tag = soup.find('a', attrs={'aria-label': 'Telegram'})

      if a_tag:
        telegram_handle = a_tag.get('href')
      else:
        telegram_handle = 'N/A'

      # 11 - Github repo
      a_tag = soup.find('a', attrs={'aria-label': 'Github'})

      if a_tag:
        github_repo = a_tag.get('href')
      else:
        github_repo = 'N/A'

      # 12 - Reddit
      a_tag = soup.find('a', attrs={'aria-label': 'Reddit'})

      if a_tag:
        reddit_url = a_tag.get('href')
      else:
        reddit_url = 'N/A'

      # 13 - Medium
      a_tag = soup.find('a', attrs={'aria-label': 'Medium'})

      if a_tag:
        medium_url = a_tag.get('href')
      else:
        medium_url = 'N/A'

      # 14 Total amount raised
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 eegbixt0 css-1jcw4ls')

      total_amount_raised = 'N/A'
      if parent_div:
        total_amount_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom css-zd7onr')
        if total_amount_div:
            total_amount_raised = total_amount_div.get_text()

      # 14 team size
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 e1azb2kh0 css-1jcw4ls')

      team_size = 'N/A'
      if parent_div:
        team_size_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom css-zd7onr')
        if team_size_div:
            team_size = team_size_div.get_text()


      # 15 - Number of clients
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 e1l5eb8a0 css-1jcw4ls')

      number_of_clients = 'N/A'
      if parent_div:
        clients_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom css-zd7onr')
        if clients_div:
            number_of_clients = clients_div.get_text()

      # 16 - Year of establishment
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 ew5h1nx0 css-1jcw4ls')

      year_of_establishment = 'N/A'
      if parent_div:
        year_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom css-zd7onr')
        if year_div:
            year_of_establishment = year_div.get_text()

      #17 Profile type
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 e4tqbn60 css-tpf8av')

      profile_type = 'N/A'
      if parent_div:
        company_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom css-zd7onr')
        if company_div:
            profile_type = company_div.get_text()

      #18 - Status
      parent_div = soup.find('div', class_='MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation0 ezqtn1f1 css-1jcw4ls')

      status = 'N/A'
      if parent_div:
        status_div = parent_div.find('div', class_='MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom ezqtn1f0 css-fsjm1')
        if status_div:
            status = status_div.get_text()

      #19 - The Key Objective
      key_objective = 'N/A'
      p_tag = soup.find('p' , class_="MuiTypography-root MuiTypography-body1 MuiTypography-alignLeft css-1h7tlwk")
      if p_tag:
        key_objective = p_tag.get_text()

      #20 - Slogan / Motto
      slogan_motto = 'N/A'
      p_tag = soup.find('p' , class_="MuiTypography-root MuiTypography-body1 MuiTypography-alignLeft css-1x3i4xu")
      if p_tag:
        slogan_motto = p_tag.get_text()

      #21 - Use cases
      use_cases = []
      div_element = soup.find('div', class_='css-a696o0 e1rt5nlw1')

      if div_element:
        span_elements = div_element.find_all('span', class_='MuiChip-label MuiChip-labelMedium css-14vsv3w')
        use_cases = [span.get_text() for span in span_elements]

      #22 - Parent organization
      parent_organization = []
      div_element = soup.find('div', class_='MuiGrid-root MuiGrid-container MuiGrid-item css-n6nfem')

      if div_element:
          span_elements = div_element.find_all('span', class_='MuiChip-label MuiChip-labelSmall css-tavflp')
          parent_organization = [span.get_text() for span in span_elements]

      #23 - Sub organizations
      sub_organization = []
      div_element = soup.find_all('div' , class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6 MuiGrid-grid-sm-6 MuiGrid-grid-md-9 css-q25qnl")
      if len(div_element) > 1:
        element_i_want = div_element[1].find_all('span', class_="MuiChip-label MuiChip-labelSmall css-tavflp")
        for every_span in element_i_want:
          sub_organization.append(every_span.get_text())
      else:
        sub_organization = 'N/A'

      # 24 - Registered Entity Name
      registered_entity_name = 'N/A'
      div_element = soup.find_all('div' , class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6 MuiGrid-grid-sm-6 MuiGrid-grid-md-9 css-q25qnl")
      if(len(div_element)) > 2:
          element_i_want = div_element[2].find('p', class_="MuiTypography-root MuiTypography-body1 css-yt6mq")
          registered_entity_name = element_i_want.get_text()

      # The block of code below was used selenium to click on the show more button to get the full profile bio
      profile_bio = 'N/A'
      async def get_profile_bio():
        async with async_playwright() as pw:
          browser = await pw.chromium.launch(
              headless=True
          )
          full_url = 'https://www.blockdata.tech/' + url

          page = await browser.new_page()
          await page.goto(full_url)
          await page.wait_for_timeout(3000)

          # # Get div for 'About' section
          about = [p for p in await page.query_selector_all(
              '.MuiPaper-root.MuiPaper-elevation.MuiPaper-rounded.MuiPaper-elevation0.MuiCard-root.css-7fmivh') if
                      'About' in await p.text_content()][0]
          
          show_more_span = await page.query_selector('.css-1o4oh22.e1pn9n6e0')

          # content = await show_more_span.text_content()
          if show_more_span:
              await show_more_span.click()  # clicking the show more span
          
          extracted_text = await about.query_selector('.MuiPaper-root.MuiPaper-elevation.MuiPaper-rounded.MuiPaper-elevation0.e7h6lww0.css-14milvj')
          
          bio = await extracted_text.text_content()

          # Find the index of "Show less"
          index = bio.find("Show less")

          # Remove "Show less" from the string
          profile_bio = bio[:index]
        return profile_bio

      #if the code is being executed in the main module. Then get_profile_bio function will execute
      if __name__ == '__main__':
        profile_bio = asyncio.run(get_profile_bio())

      # writing extracted info to the csv file
      about_section_writer.writerow([profile_link, company_name,profile_bio,
                         image_url, company_caption,
                         company_description, based_in, website_url,
                         linked_in_profile, twitter_handle,facebook_page,
                         telegram_handle, github_repo, reddit_url,medium_url,
                         total_amount_raised,team_size,number_of_clients,year_of_establishment,
                         profile_type,status, key_objective, slogan_motto, use_cases, parent_organization,sub_organization, registered_entity_name ])

      # Beginning scrap of the team section
      # 25 - Team members section
      team_members = []
      grid_element = soup.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-2 css-5g8fo7")
      if grid_element:
        single_element_in_grid = grid_element.find_all('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-sm-6 css-rpybyc")
        if(single_element_in_grid):
          for element in single_element_in_grid:
            team_member_detail_div = element.find('div', class_="MuiPaper-root MuiPaper-outlined MuiPaper-rounded css-1avv7jm")
            container_div_with_all_details_for_team_member = team_member_detail_div.find('div', class_="MuiGrid-root MuiGrid-container css-1d3bbye")
            linkedIn = container_div_with_all_details_for_team_member.find('a', class_='MuiButtonBase-root MuiIconButton-root MuiIconButton-colorPrimary MuiIconButton-sizeLarge erjqhew0 css-q372t6')
            if linkedIn:
              linkedIn = linkedIn["href"]
            # First get their name and last their linked in
            member_name_and_responsibility = container_div_with_all_details_for_team_member.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-direction-xs-column MuiGrid-grid-xs-true css-1injdr0")
            if member_name_and_responsibility:
              name = member_name_and_responsibility.find('span', class_="MuiTypography-root MuiTypography-body3 MuiTypography-gutterBottom css-1d2t6we")
              name_and_job_responsibility = name.get_text()
              name_responsibility_and_linkedin_combined_into_list = [name_and_job_responsibility, linkedIn]
              team_members.append(name_responsibility_and_linkedin_combined_into_list)
      else:
        team_members = 'N/A'

      ## Code for scrapping the tokens and funding section below

      # Tokens
      #26 - Tokens
      # Token Info Array
      token_info = [] # List which will store the tokens info
      token_names = []
      tokens_section = soup.find('div', class_="MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation1 MuiCard-root index-root css-1tgiuf5")
      if tokens_section:
        sub_tokens_section_div = tokens_section.find('div', class_="MuiCardContent-root index-content css-1qw96cp")
        if sub_tokens_section_div:
          flex_container_containing_token_info = sub_tokens_section_div.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-3 index-cardContentGrid css-s2t2cz")
          individual_div_containing_token_info = flex_container_containing_token_info.find_all('div', class_="MuiGrid-root MuiGrid-item css-1wxaqej")
          if individual_div_containing_token_info:
            for each_div in individual_div_containing_token_info:
              p_tag = each_div.find('p', class_="MuiTypography-root MuiTypography-body1 e1ontwvu1 css-1jeqi5i")
              if p_tag:
                token_names.append(p_tag.get_text())
              tag_h6 = each_div.find('h6' , class_="MuiTypography-root MuiTypography-subtitle2 css-1whifbp")
              if tag_h6:
                tag_h6 = tag_h6.get_text()
              token_info.append(tag_h6) #appending the extracted info to the list
      else:
        token_info = 'N/A'

      # href url which the info on the token can be found
      link_to_token = 'N/A'
      if tokens_section:
        a_tag_container = tokens_section.find('div', class_="MuiCardActions-root css-jo6j44")
        if a_tag_container:
          a_tag = a_tag_container.find('a', class_="MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeSmall MuiButton-textSizeSmall MuiButtonBase-root css-x16yd0")
          if a_tag:
            link_to_token = a_tag["href"]
            base_url_for_tokens = "https://www.blockdata.tech"
            link_to_token = base_url_for_tokens + link_to_token
            token_info.append(link_to_token)

      # Now for scrapping the funding section
      total_amount_raised = 'N/A'
      rounds = 'N/A'
      total_amount_of_investors = 'N/A'
      funding_sources = []
      funding_section = soup.find('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 ehdjfxk0 css-1lk7s4z")
      if funding_section:
        sub_section = funding_section.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-8 css-1l5mznc")
        if sub_section:
          first_container = sub_section.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-3 css-1ofn76x")
          if first_container:
            items_list_in_the_first_container = first_container.find_all('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6 MuiGrid-grid-sm-4 css-idrrft")
            if items_list_in_the_first_container:
              #1 - Total amount raised
              if len(items_list_in_the_first_container) > 1:
                #I want to take the fist item in the list
                item = items_list_in_the_first_container[0].find('div', class_="MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom index-count css-1axqeeq")
                total_amount_raised = item.get_text()

              #2 - Rounds
              if len(items_list_in_the_first_container) > 2:
                #I want to take the fist item in the list
                item = items_list_in_the_first_container[1].find('div', class_="MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom index-count css-1axqeeq")
                rounds = item.get_text()


              #3 - Rounds
              if len(items_list_in_the_first_container) >= 3:
                #I want to take the fist item in the list
                item = items_list_in_the_first_container[2].find('div', class_="MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom index-count css-1axqeeq")
                total_amount_of_investors = item.get_text()

        #2 - Funding sources
        source_object = {}
        sources = []
        second_sub_section = funding_section.find('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 css-15j76c0")
        if second_sub_section:
          inner_section = second_sub_section.find('div', class_="css-1akhy6d e5y0f1k0")
          if inner_section:
            table_with_sources_of_funding = inner_section.find('div', class_="tbody")
            if table_with_sources_of_funding:
              rows = table_with_sources_of_funding.find_all('div', class_="tr")
              for row in rows:
                td = row.find_all('div', class_="td")
                amount_raised = td[0].get_text()
                funding_type = td[1].get_text()
                funding_round_closing = td[2].get_text()
                investors_ = td[3].find_all('span', class_="MuiChip-label MuiChip-labelSmall css-tavflp")
                list_of_investors = []
                if len(investors_) > 1:
                  for investor in investors_:
                    list_of_investors.append(investor.get_text())
                elif len(investors_) == 1:
                  investors_ = td[3].get_text()
                  list_of_investors.append(investors_)
                else:
                  list_of_investors.append('N/A')

                source_object = {
                  'Amount Raised' : amount_raised,
                  'Funding Type' : funding_type,
                  'Funding Round Close' : funding_round_closing,
                  'Investors' : list_of_investors
                }

                sources.append(source_object)
        # Getting the investment info.
        investments2 = 'N/A'
        investment_rounds = 'N/A'
        total_amount_of_profiles = 'N/A'
        investment_section = soup.find('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 e15qxe2m0 css-1o2y007")
        if investment_section:
          sub_section2 = investment_section.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-8 css-1l5mznc")
          if sub_section2:
            sub_section3 = sub_section2.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-3 css-1ofn76x")
            if sub_section3:
              items_list_in_the_first_container1 = sub_section3.find_all('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-6 css-1s50f5r")
              if items_list_in_the_first_container1:
                #1 - Total amount raised
                if len(items_list_in_the_first_container1) > 1:
                  #I want to take the fist item in the list
                  item = items_list_in_the_first_container1[0].find('div', class_="MuiPaper-root MuiPaper-outlined MuiPaper-rounded index-paperBox css-f7fxqs")
                  if item:
                    item = item.find('div', class_="MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom index-count css-1axqeeq")
                    investment_rounds = item.get_text()
                if len(items_list_in_the_first_container1) >= 2:
                  #I want to take the fist item in the list
                  item = items_list_in_the_first_container1[1].find('div', class_="MuiPaper-root MuiPaper-outlined MuiPaper-rounded index-paperBox css-f7fxqs")
                  if item:
                    item = item.find('div', class_="MuiTypography-root MuiTypography-h6 MuiTypography-gutterBottom index-count css-1axqeeq")
                    total_amount_of_profiles = item.get_text()
        else:
          investment_rounds = 'N/A'
          total_amount_of_profiles = 'N/A'

        investment_sources = []
        investment_object = {}
        second_sub_section_of_funding = soup.find('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 e15qxe2m0 css-1o2y007")
        if second_sub_section_of_funding:
          container_with_table = second_sub_section_of_funding.find('div', class_="MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-8 css-1l5mznc")
          if container_with_table:
            container_with_table = container_with_table.find('div', class_="MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 css-15j76c0")
            if container_with_table:
              container_with_table = container_with_table.find('div', class_="css-1hqnz7 e1xvby5o1")
              if container_with_table:
                table = container_with_table.find('div', class_="table")
                if table :
                  table_with_sources_of_investments = table.find('div', class_="tbody")
                  if table_with_sources_of_investments:
                    rows = table_with_sources_of_investments.find_all('div', class_="tr pointer")
                    for row in rows:
                      td = row.find_all('div', class_="td")
                      name = td[1].get_text() #1 --- Name

                      industries = td[2].find_all('span', class_="MuiChip-label MuiChip-labelSmall css-tavflp")
                      list_of_industries = [] #2 --- Industries
                      if len(industries) > 1:
                        for industry in industries:
                          list_of_industries.append(industry.get_text())
                      elif len(industries) == 1:
                        industry = td[2].get_text()
                        list_of_industries.append(industry)
                      else:
                        list_of_industries.append('N/A')
                      investment_use_cases = td[3].find_all("span", class_="MuiChip-label MuiChip-labelSmall css-tavflp")

                      list_of_investment_use_cases = [] #3 --- Investment use cases
                      if len(investment_use_cases) > 1:
                        for the_use_case in investment_use_cases:
                          list_of_investment_use_cases.append(the_use_case.get_text())
                      elif len(investment_use_cases) == 1:
                        the_use_case = td[3].get_text()
                        list_of_investment_use_cases.append(the_use_case)
                      else:
                        list_of_investment_use_cases.append('N/A')

                      total_raised = td[4].get_text() #4 --- Total Raised

                      last_funding_round_date = td[5].get_text() #5 --- Last funding round data

                      investment_object = {
                        'Investment Name' : name,
                        'Investment Industries' : list_of_industries,
                        'Investment Use Cases' : list_of_investment_use_cases,
                        'Total Raised' : total_raised,
                        'Last Funding Round Date' : last_funding_round_date
                      }
                      investment_sources.append(investment_object)

      # un indented below
      funding_section_writer.writerow([profile_link, company_name, total_amount_raised, rounds,
              total_amount_of_investors, sources, investment_rounds, total_amount_of_profiles, investment_object])

      # Code below writes 
      team_section_writer.writerow([profile_link, company_name, team_members])

      if len(token_info) >= 8:
        tokens_section_writer.writerow([profile_link, company_name, token_names[0], token_info[1], token_info[2], token_info[3], token_info[4], token_info[5], token_info[6], token_info[7]])
      else:
        tokens_section_writer.writerow([profile_link, company_name] + [token_info[i] if i < len(token_info) else 'N/A' for i in range(8)])



      time.sleep(1)
